<?xml version="1.0" encoding="utf-8"?>
<research title="Selected Measures and Research Methods" description="This tool is designed to find the correct method and standardized operationalization of concepts for your HCI research. This tool only offers tips and help for beginners for quick start and is not a substitute for extensive research or input of your supervisor.">
	<options type="dropdown" id="types" question="What is the purpose of your study?">
		<option id="formative" next="designdecision">Designing or improving a system, interaction, or method</option>
		<option id="summative" next="evidences">Testing and understanding devices or people</option>
		<option id="literature" next="literaturereviews">Reviewing and summarizing scientific literature</option>
	</options>
	<options type="dropdown" id="literaturereviews" question="Why do you want to read the literature?">
		<option id="narrative">Finding research streams, frameworks, gaps, inconsistencies (Traditional Reviews)</option>
		<option id="systematic">Answer one specific research question (Systematic Reviews)</option>
		<option id="scoping">Identifying research scope and answer a general research question (Scoping Reviews)</option>
		<option id="metaanalysis">Summarizing results of empirical studies addressing the same question (Meta-Analysiss)</option>
	</options>
	<options type="dropdown" id="designdecision" question="Progess in your design process?">
		<option id="solutions" next="prototyping">Start the design from scratch (Solution-oriented)</option>
		<option id="problems" next="evidences">Identify the problems with something that exists (Problem-oriented)</option>
	</options>
	<options type="dropdown" id="prototyping" question="Are you interested in the design process or outcome?">
		<option id="designprocess">Interested in the desigh process or design space (Design-Method)</option>
		<option id="prototype">Interested in the final outcome (Prototyping)</option>
	</options>
	<options type="dropdown" id="evidences" question="What scientific evidence is your research based on?">
		<option id="analytical" next="analyticaldecisions">Based on analysis and logic (Analytical Evidence)</option>
		<option id="empirical" next="sample">Based on experience by observation (Empirical Evidence)</option>
	</options>
	<options type="dropdown" id="sample" question="On which basis do like to draw your sample?">
		<option id="single" notIf="summative">A single case (one person, one event, one location, one organization, ...)</option>
		<option id="experts" notIf="summative">A special subset (selected experts, designers, events, locations...)</option>
		<option id="randomsample" notIf="formative" next="method">A random/representative sample from a population (people, groups, events, locations,...)</option>
	</options>
	<options type="dropdown" id="analyticaldecisions" question="On what is your analysis based on?">
		<option id="datadriven">Data (data-driven)</option>
		<option id="modeldriven">Models (model-driven)</option>
	</options>
	<options type="checkbox" id="method" question="How do you draw your sample?">
		<option id="quantitative" next="datadecision">Measuring numerical values (Quantitative Methods)</option>
		<option id="qualitative" next="datadecision">Collecting non-numeric feedback (Qualitative Methods)</option>
	</options>
	<options type="checkbox" id="datadecision" question="Where does your data come from?">
		<option id="objective" onlyIf="quantitative" next="concept">From a measuring instrument (Objective data)</option>
		<option id="subjective"  onlyIf="quantitative" next="concept">From a statement of a person (Subjective data)</option>
	</options>
	<options type="checkbox" id="concept" question="Which concept do you want to measure?">
		<option id="demographics" alwaysOn="on" disabled="on">Demographics (required)</option>
		<option id="attention" onlyIf="objective">Attention</option>
		<option id="emotions" >Emotions</option>
		<option id="ergonomics">Ergonomics</option>
		<option id="fitness">Fitness</option>
		<option id="flow" onlyIf="subjective">Flow</option>
		<option id="hedonicpragmatic" onlyIf="subjective">Fun</option>
		<option id="games" onlyIf="subjective">Gaming Experience</option>
		<option id="humanlikeness" onlyIf="subjective">Human-Likeness</option>
		<option id="immersion" onlyIf="subjective">Immersion</option>
		<option id="presence" onlyIf="subjective">Presence</option>
		<option id="bodyownership" onlyIf="subjective">Body-Ownership</option>
		<option id="health" onlyIf="subjective">Health</option>
		<option id="learning" onlyIf="objective">Learning</option>
		<option id="longitudinal" onlyIf="subjective">Longitudinal Experience</option>
		<option id="memory" onlyIf="objective">Memory</option>
		<option id="motivation" onlyIf="subjective">Motivation</option>
		<option id="movement" onlyIf="objective">Movement</option>
		<option id="performance" onlyIf="objective">Performance</option>
		<option id="workload" onlyIf="subjective">Workload</option>
		<option id="sensualacuity">Sensual Acuity</option>
		<option id="stress">Stress, Excitement, and Relaxation</option> 
		<option id="mentalworkload">Mental Workload</option>
		<option id="socialacceptance" onlyIf="subjective">Social Acceptance</option>
		<option id="usability" onlyIf="subjective">Usability</option>
		<option id="userexperience" onlyIf="subjective">User Experience</option> 
	</options>
	<measures>
		<method id="demographics" category="summative,demographics" title="Demographics">
			Always please record for each individual participating in your study:
			<div class="list-group">
				<div class="list-group-item"><div><div class="fw-bold">Gender</div> <div>Options: male, female, other, do not want to specify</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Age</div><div>Options: numerical input from 0 to N</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Highest Academics Degree</div><div>Options: none, primary school, secondary school, post-secondary school diploma / high school diploma, completed vocational training, completed university studies, doctoral degree</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Occupation</div><div>Options: Management, Natural Sciences, Engineering, Health, Law and Order, Clerical, Spiritual, Social, Humanities, Services and Sales, Agriculture and Food, Craft, Military, Others</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Nationality</div><div>Options: <a href="country.txt">Get full list</a></div></div></div>
			</div>
			<p class="mt-2">More participant-related information could be: </p>
			<div class="list-group">
				<div class="list-group-item"><div><div class="fw-bold">Expertise</div><div>What is special about your participants (experts in their field? students? ...)</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Trial Time</div><div>Experimental trial time for each participant</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Compensation</div><div>â‚¬, credit points for the lecture</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Handicaps</div><div>Disease, disorder, illness, anxiety...</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Ethnical/Cultural/Racial Background</div><div>Only when this is important for your study...</div></div></div>
			</div>
		</method>

		<method id="skinconductance" category="empirical,quantitative,objective,stress" title="Quantitative Objective Stress: Skin Conductance Response (SCR)/Electro Dermal Activity (EDA)">
			<p>Skin Conductance Response (SCR) is a measure of the electrical conductance of the skin. It is used as an indicator of sympathetic nervous system activity, which is related to emotional or physiological arousal. SCR is typically measured by attaching electrodes to the skin, usually on the fingers or palms, and recording the electrical conductance between them. The electrodes are connected to a device that amplifies and records the electrical signals.  </p><p>The resulting data can then be analyzed to determine the level of SCR, which can be used to infer changes in emotional or physiological arousal in response to various stimuli. SCR is commonly used in research on emotion, cognition, and stress, as well as in clinical applications such as lie detection and biofeedback.</p> Source: <a href="https://en.wikipedia.org/wiki/Electrodermal_activity" target="_blank">Wikipedia</a>
		</method>

		<method id="heartratevariablity" category="empirical,quantitative,objective,stress" title="Quantitative Objective Stress: Heart Rate Variability (HRV)">
			<p>Heart Rate Variability (HRV) is a measure of the variation in the time interval between successive heartbeats. It is typically measured by recording the electrocardiogram (ECG) signal, which is the electrical activity of the heart. HRV reflects the balance between the sympathetic and parasympathetic nervous systems, which control the heart rate. A high HRV indicates a healthy balance between the two systems, while a low HRV indicates an imbalance, which may be associated with stress, anxiety, or other health conditions.</p><p>HRV analysis can be used to assess the overall health of the cardiovascular system, as well as to monitor the effects of interventions such as exercise, medication, or stress management. It is also used in research on physiological regulation, psychophysiology, and health care. Source:</p> <a href="https://en.wikipedia.org/wiki/Heart_rate_variability" target="_blank">Wikipedia</a>
		</method>

		<method id="jnd" category="empirical,quantitative,objective,sensualacuity" title="Quantitative Objective Sensual Acuity: Just-Noticeable Difference (JND)">
			<p>Just-Noticeable Difference (JND) is a concept in psychophysics that refers to the minimum amount of change in a stimulus that is needed for it to be perceived as different from the original stimulus. It is the smallest difference in the intensity, duration, or other physical characteristics of a stimulus that can be detected by the observer. The JND is determined by the characteristics of the observer and the properties of the stimulus. For example, the JND for a visual stimulus is dependent on the properties of the observer's eyes, the lighting conditions, and the characteristics of the stimulus.</p><p>JND is used in many fields such as product design, marketing, and human-computer interaction to understand how small changes in a product or design can be perceived by users. It also has a great importance in research on sensation and perception as it allows to quantify the sensitivity of the human senses. </p>Source: <a href="https://en.wikipedia.org/wiki/Just-noticeable_difference" target="_blank">Wikipedia</a>
		</method>

		<method id="borg" category="empirical,quantitative,subjective,fitness" title="Quantitative Subjective Fitness: Rating of Perceived Exertion (RPE) / 'Borg Scale'">
			<p>Rating of Perceived Exertion (RPE) is a scale used to measure an individual's subjective feeling of effort or exertion during physical activity. It is a simple and reliable tool that allows individuals to rate their effort on a scale of 0 to 20 or 0 to 10, with higher numbers indicating a greater feeling of exertion. The most commonly used RPE scale is the Borg Scale, which ranges from 6 to 20. It is based on the individual's perception of how hard they are working and how much effort they are putting in.</p><p>RPE can be used to monitor exercise intensity, track changes in fitness, and evaluate the effectiveness of exercise programs. It is a simple, non-invasive, and widely used method in exercise physiology, sports science, and rehabilitation. RPE is also used in research on exercise and physical activity, as well as in clinical settings to monitor patients with chronic conditions such as heart disease and diabetes.</p> Source: <a href="https://en.wikipedia.org/wiki/Rating_of_perceived_exertion" target="_blank">Wikipedia</a>
		</method>

		<method id="gripstrength" category="empirical,quantitative,objective,fitness" title="Quantitative Objective Fitness: Grip Strength (GS)">
			<p>Grip Strength (GS) is a measure of the maximal force that an individual can exert with their hand or hand-held device. It is typically measured using a hand-held dynamometer, which is a device that measures force when an individual squeezes it with their hand. GS is a widely used measure of upper body strength and it is a simple, reliable and valid measure of muscle function. It is often used to assess the muscle strength of the hand, wrist, and forearm.</p><p>It has been used in many fields such as rehabilitation, sports science, geriatrics, and occupational therapy. It is also used as a predictor of overall health and functional ability, as well as to monitor the effectiveness of exercise and physical therapy programs. In research, it is used to investigate the effects of aging, disease, injury, and other factors on muscle function.</p>Source: <a href="https://en.wikipedia.org/wiki/Grip_strength" target="_blank">Wikipedia</a>
		</method>

		<method id="heartrate" category="empirical,quantitative,objective,stress" title="Quantitative Objective Stress: Heart Rate (HR)">
			<p>Heart Rate (HR) is used as a measure of physiological and emotional responses of users interacting with digital devices. HR is widely used as an indicator of emotional and cognitive states, such as stress, excitement, and mental or physical workload. By measuring the HR, researchers can gain insight into how users respond to different interfaces, designs, or features of a digital device, such as mobile phones, tablets, and computers.</p><p>In HCI research, HR measurements can be used in combination with other physiological measurements such as skin conductance, galvanic skin response, and electromyography. This allows researchers to gain a more comprehensive understanding of the user's physiological and emotional responses to different digital devices and interfaces. HR can also be used to evaluate the effectiveness of interventions such as stress management, biofeedback or mindfulness training programs.</p>Source: <a href="https://en.wikipedia.org/wiki/Heart_rate" target="_blank">Wikipedia</a>
		</method>

		<method id="usabilitytest" category="empirical,problems,randomsample" title="Qualitative Feedback: Usability test with users">
			<p>A usability test is a method used to evaluate the usability of a digital product or system by testing it with users. The goal of a usability test is to identify any problems or issues that users may encounter when using the product or system, and to gather feedback on its overall usability. During a usability test, participants are asked to perform a set of tasks while interacting with the product or system being tested. These tasks are designed to simulate real-world scenarios and are chosen to represent the most important and common activities that users will perform.</p><p>The participants are typically asked to think aloud as they complete the tasks, so that the researchers can understand their thought process and any difficulties they encounter. The usability test is typically conducted in a controlled environment, such as a lab, where the participants' interactions with the product or system can be observed and recorded.</p><p>The test is usually conducted with a small number of participants, usually between 5 to 8, and can be conducted in-person or remotely. The results of the usability test are analyzed to identify any problems or issues that users encountered, and to gather feedback on the overall usability of the product or system. Based on the results, design recommendations can be made to improve the usability of the product or system.</p>Source: <a href="https://en.wikipedia.org/wiki/Usability_testing" target="_blank">Wikipedia</a>
		</method>

		<method id="publicstats" category="analytical,datadriven" title="Data-driven Analysis: Official/Public Statistics">
			<p>Data-driven analysis is a method of using statistical and computational techniques to extract insights and knowledge from data. Official statistics, also known as public statistics, refer to data that is collected and published by government agencies or other official sources. These sources include national statistical agencies, central banks, and other government departments. Official statistics are used in various fields such as economics, social sciences, and public policy. They are considered to be reliable and unbiased, as they are collected and published by neutral and independent organizations.</p><p>In Data-driven Analysis, Official statistics are used as a primary source of data in many research projects and studies. They provide a wealth of information on a wide range of topics such as population, income, education, crime, health, and more. This data can be used to analyze trends, patterns, and relationships between different variables, and to make predictions about future developments.</p><p>In addition to being a valuable source of data, official statistics are also used to validate and benchmark results obtained from other data sources, such as surveys and experiments. For example, official statistics on GDP can be used to validate results from a survey on consumer spending, or official statistics on crime can be used to validate results from an experiment on the effectiveness of a new policing strategy.</p> Source: <a href="https://en.wikipedia.org/wiki/Official_statistics" target="_blank">Wikipedia</a>
		</method>

		<method id="ethnography" category="analytical,datadriven" title="Data-driven Analysis: Ethnography/Autoethnography">
			<p>Ethnography and Autoethnography are research methods that are used in the field of Human-Computer Interaction (HCI) to understand the cultural and social aspects of technology use.</p><p>Ethnography in HCI involves observing and studying the ways in which people use technology in their everyday lives. It typically involves conducting fieldwork, such as visiting people in their homes or workplaces, to observe how they interact with technology and understand the context in which technology is used. Ethnography in HCI can provide valuable insights into how technology is integrated into people's lives, and how it shapes their social interactions and behaviors.</p><p>Autoethnography is a variation of ethnography that involves the researcher being a part of the culture or community being studied. In HCI, Autoethnography can be used to examine the researcher's own experiences and understanding of technology use. It allows the researcher to explore their own relationship with technology, and to reflect on how it shapes their own behaviors and interactions. Autoethnography allows researcher to understand the personal experience with the technology which can't be captured by traditional ethnography methods.</p><p>Both ethnography and autoethnography are useful methods for understanding the social and cultural dimensions of technology use in HCI. They can provide insights into how technology is integrated into people's lives, how it shapes their behaviors and interactions, and how it is perceived and experienced by different groups of people.</p>Source: <a href="https://en.wikipedia.org/wiki/Ethnography" target="_blank">Wikipedia</a>
		</method>

		<method id="diary" category="empirical,qualitative,subjective,longitudinal" title="Qualitative Feedback: Diary Study with Experience Sampling Method (ESM)">
			<p>Diary studies are a research method used in Human-Computer Interaction (HCI) to study people's experiences and behaviors over time. Diary studies involve asking participants to document their thoughts, feelings, and activities in a diary, journal, or other form of written record over a period of time, typically days or weeks..</p><p>In HCI, diary studies are used to understand people's everyday experiences with technology, such as how they use it, what problems they encounter, and how it impacts their lives. They can provide insights into how technology is integrated into people's daily routines, how it shapes their behaviors, and how it affects their emotions and well-being.</p><p>Diary studies can be used in a variety of settings, such as in the workplace, at home, or in educational settings. They can be used to study a wide range of technology-related topics, such as mobile phone use, social media, and internet usage..</p><p>Participants are usually asked to complete the diary entries at regular intervals, such as at the end of each day. Participants are also usually asked to complete a final survey or interview to provide more detailed information about their experiences. The diary entries and final survey/interview data can be analyzed together to gain a more complete understanding of the participants' experiences..</p><p>Diary studies are a valuable method in HCI research, as they provide rich and detailed data about people's everyday experiences with technology. They can provide insights that are difficult to obtain through other methods, such as surveys or interviews, and can be particularly useful for understanding how technology is used in naturalistic settings.</p>Source: <a href="https://www.nngroup.com/articles/diary-studies/" target="_blank">Nielsen Norman Group</a>
		</method>

		<method id="scnearioofuse" category="empirical,single" title="Qualitative Investigation: Scenario of Use / Case Study / Use Case Study">
			<p>All three methods, Scenario of Use, Case Study and Use Case Study, are similar approaches used to understand how people interact with technology in very specific contexts, and to inform the design and evaluation of technology. They provide detailed and in-depth insights into how people use technology in their everyday lives and help to identify potential issues and areas for improvement.</p><p>A Scenario of Use is a description of a specific situation or context in which a product or system is used. It describes the goals, actions, and interactions of the user, as well as the context in which the product or system is used. Scenarios of use are used to inform the design of a product or system, and to evaluate its usability and effectiveness.</p>  <p>A Case Study is a research method that involves in-depth examination of a specific instance or example of a phenomenon. In HCI, case studies are used to understand the specific experiences and behaviors of individuals or groups in relation to a particular technology or system. They typically involve collecting a variety of data, such as interviews, observations, and documentation, and analyzing it to gain a detailed understanding of the case.</p>  <p>A Use Case Study is a specific type of case study that focuses on the use of a particular product or system. It involves studying how people use a product or system in real-world settings, and understanding the tasks, goals, and interactions that are associated with its use. Use case studies are used to evaluate the usability and effectiveness of a product or system, and to identify potential design improvements.</p>Source: <a href="https://www.nngroup.com/articles/quantifying-case-study/" target="_blank">Nielsen Norman Group</a>
		</method>

		<method id="postinterview" category="empirical,qualitative,subjective,randomsample" title="Qualitative Feedback: Semi-Structured Interview">
			<p>A Semi-Structured Interview is a research method used in Human-Computer Interaction (HCI) to gather qualitative data about people's experiences, attitudes, and behaviors in relation to technology. It is a type of interview that combines the flexibility of an unstructured interview with the structure of a structured interview.</p><p>In a semi-structured interview, the interviewer has a set of predetermined questions or topics to guide the conversation, but also allows the interviewee to elaborate on their answers and to introduce new topics. This allows the interviewee to provide in-depth and detailed responses, while still ensuring that the interview covers the key topics of interest.</p><p>Semi-structured interviews are typically conducted in person or over the phone, and are usually recorded for transcription and analysis. They can be used to gather data about a wide range of topics related to technology, such as people's experiences with specific products or systems, their attitudes towards technology in general, or their behaviors and practices related to technology use.</p><p>Semi-structured interviews are a valuable method in HCI research, as they provide rich and detailed data about people's experiences, attitudes, and behaviors related to technology. They can be particularly useful for understanding the subjective experiences of participants and for gaining insights into how people use technology in their everyday lives. They are also useful in obtaining more in-depth and nuanced data compared to survey-based methods.</p>Source: <a href="https://en.wikipedia.org/wiki/Semi-structured_interview" target="_blank">Wikipedia</a>
		</method>

		<method id="thinkaloudexperts" category="formative,empirical,problems,experts" title="Qualitative Feedback: Expert Think-Aloud">
			<p>The Think-Aloud method can also be used with experts, such as experts in a particular field or domain, or experts in the design and development of technology. This is known as Expert Think-Aloud.</p><p>When using the Think-Aloud method with experts, the researcher may ask the expert to perform a task or use a product or system while verbalizing their thoughts, just like with non-experts. However, in this case, experts are able to provide more in-depth and detailed verbalization, as they have more knowledge and experience in the specific field or domain.</p><p>Expert Think-Aloud can be particularly useful in evaluating the design and functionality of a product or system, as experts may have a more nuanced understanding of the user needs and requirements. It can also be used to identify areas for improvement, and to gather expert opinions on the design, usability, and functionality of a product or system.</p><p>However, it is important to keep in mind that experts may have different perspective than the end-users, and their verbalization may not always align with the end-users. Therefore, it is important to collect data from both experts and non-experts to gain a comprehensive understanding of the user's experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Think_aloud_protocol" target="_blank">Wikipedia</a>
		</method>

		<method id="thinkaloud" category="empirical,qualitative,subjective,randomsample" title="Qualitative Feedback: Think-Aloud">
			<p>to understand how people interact with digital products or systems, and to identify usability problems. It is a method of verbal protocol analysis in which participants are asked to think aloud, or verbalize their thoughts, as they perform a task or use a product or system. The method involves having participants speak out loud about what they are doing and thinking as they interact with the technology. This verbalization provides insight into the user's cognitive process as they interact with the technology and allows researchers to identify any problems or difficulties the user may encounter. This method is widely used in usability testing and user-centered design, as it allows researchers to identify usability issues and make design recommendations to improve the user experience.</p><p>The Think-Aloud method typically involves the following steps:</p><ul><li>  Participants are asked to complete a set of tasks while interacting with a product or system.</li><li> The verbalizations are recorded, either through audio or video recording, and later transcribed for analysis. </li><li>Researchers analyze the verbalizations to understand how participants interact with the technology, and identify any problems or difficulties they encounter.</li></ul> <p>The Think-Aloud method allows researchers to understand the user's thought process while they interact with the technology and provide insight into how they are interpreting and making sense of the interface. This method can be used as a supplement to other methods such as usability testing, surveys, and interviews, to gain a comprehensive understanding of the user's experience.</p><a href="https://en.wikipedia.org/wiki/Think_aloud_protocol" target="_blank">Wikipedia</a>
		</method>

		<method id="cit" category="empirical,problems,experts" title="Qualitative Feedback: Critical Incident Technique (CIT)">
			<p>The Critical Incident Technique (CIT) is a research method used to gather qualitative data about specific events or incidents that are considered critical or important in some way. CIT is widely used in various fields, including Human-Computer Interaction (HCI) to understand how people interact with technology, and to identify and analyze critical incidents related to technology use.</p><p>The CIT method involves asking participants to report on incidents that they have experienced related to the topic being studied. These incidents can be positive or negative, and can range from minor frustrations to major failures. Participants are asked to provide detailed descriptions of the incidents, including what happened, when, where, and how they felt about it.</p><p>Once the incidents are collected, they are analyzed and coded to identify common themes and patterns. The data can then be used to identify and understand critical incidents related to technology use, and to make design recommendations or to improve the usability and effectiveness of a product or system.</p><p>CIT is a valuable method in HCI research, as it allows researchers to gather rich and detailed data about critical incidents related to technology use. It can provide insights into how people interact with technology, and can be used to identify usability problems and design solutions. CIT is particularly useful when combined with other methods, such as surveys, interviews and usability testing, to gain a comprehensive understanding of the user's experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Critical_incident_technique" target="_blank">Wikipedia</a>
		</method>

		<method id="affinitydiagramming" category="formative,solutions,designprocess" title="Design Method: Affinity Diagram">
			<p>Affinity diagramming is a method used to organize and categorize information gathered during research or brainstorming sessions. It is a technique that helps to group ideas, thoughts, or data into related categories and identify patterns and connections.  The process of affinity diagramming begins by collecting a large amount of information, such as notes from interviews, observations, or brainstorming sessions.</p><p>This information is then written on sticky notes or cards and placed on a wall or board. Participants then work together to group the notes into related categories by moving the notes around and discussing the connections between them.  Once the notes are grouped into categories, the categories are given labels or names that summarize the main theme or idea of the group. These labels are then organized into a hierarchy, with higher-level categories representing broader themes and lower-level categories representing more specific ideas.</p><p>The output of affinity diagramming is a visual representation of the information that is easy to understand and navigate. It helps to identify patterns and connections in the data, and can be used to inform the design of products or services, generate ideas for research, or guide decision-making.  Affinity diagramming is a useful tool for bringing structure to large amounts of unstructured data, and it allows the group to work collaboratively and identify patterns and connections that might not be apparent otherwise. It's a powerful method to use in a team setting as it allows the team to have a shared understanding of the research or project.</p>Source: <a href="https://en.wikipedia.org/wiki/Affinity_diagram" target="_blank">Wikipedia</a>
		</method>

		<method id="participatorydesign" category="formative,solutions,designprocess" title="Design Method: Participatory Design">
			<p>Participatory design (PD) is a method used in human-computer interaction (HCI) and user-centered design to involve users in the design process. It is a technique that involves engaging users as active participants in the design process, rather than passive recipients of a design. This allows users to contribute their knowledge, skills, and perspectives to the design, leading to more usable and user-centered products.</p><p>Participatory design can take many forms, such as co-design sessions, workshops, focus groups, or ethnographic research. The goal of PD is to facilitate a collaborative design process between users and designers, where users and designers work together to understand the user's needs, goals, and constraints and to create designs that are tailored to their needs.</p><p>The process of participatory design typically involves creating a collaborative design environment where users and designers can work together to understand the user's needs, goals, and constraints, and to create designs that are tailored to their needs. This can involve using a variety of methods, such as brainstorming, affinity diagramming, card sorting, and prototyping.</p><p>Participatory design is a valuable tool for HCI designers because it allows them to involve users in the design process, which leads to more usable and user-centered products. It also allows designers to gain a deeper understanding of the user's needs, goals, and constraints, which can inform the design of the product or service. </p>Source: <a href="https://en.wikipedia.org/wiki/Participatory_design" target="_blank">Wikipedia</a>
		</method>

		<method id="physicalergonomics" category="formative,solutions,designprocess" title="Design Method: Physical Ergonomics">
			<p>Physical ergonomics is a sub-discipline of ergonomics that focuses on the physical aspects of human interactions with products, equipment, and environments. It aims to design and optimize products and environments to fit the physical capabilities and limitations of the people who use them.</p><p>Physical ergonomics is concerned with the design of products and environments to minimize physical strain, discomfort, and injury, and to promote ease of use, safety and productivity. It is applied to a wide range of products and environments, including furniture, equipment, tools, vehicles, buildings, and outdoor spaces.  Examples of physical ergonomics in practice include designing office chairs that adjust to the user's body shape, designing car interiors that accommodate different body sizes and shapes, and designing tools and equipment that are comfortable and easy to hold and use.</p><p>Physical ergonomics involves the application of anthropometry, which is the study of human body measurements, and biomechanics, which is the study of the mechanics of the human body. It also involves the use of tools such as anthropometric measurement data, biomechanical models, and simulation software to design and evaluate products and environments.</p><p>Physical ergonomics is essential for the design of products and environments that are safe, comfortable, and easy to use, especially for people with disabilities, older adults, and other populations with unique physical characteristics and needs.</p>Source: <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics#Physical_ergonomics" target="_blank">Wikipedia</a>
		</method>

		<method id="paralleldesign" category="formative,solutions,designprocess" title="Design Method: Parallel Design">
			<p>Parallel design is a method used in human-computer interaction (HCI) and user-centered design to simultaneously explore multiple design options and evaluate them with users. It is a technique that involves creating multiple design concepts in parallel and testing them with users to identify the best design solution.</p><p>The process of parallel design typically involves creating multiple design concepts, each of which addresses a specific design problem or goal. These concepts are then evaluated with users, who provide feedback on their preferences, strengths, and weaknesses. This allows the design team to identify the best design solution and make adjustments as needed.</p><p>Parallel design can be done using a variety of techniques, such as paper prototyping, wireframing, or digital prototyping. The choice of technique will depend on the goals of the evaluation, the resources available, and the level of detail required.  During the testing, users interact with the design concepts and the designer or facilitator can observe and note any issues or problems that arise. The design concepts can be used to test usability, user flow, and overall user experience.  Parallel design is a valuable tool for HCI designers because it allows them to explore multiple design options simultaneously, and evaluate them with users to identify the best design solution. It also allows designers to make changes and iterate on the design quickly, based on feedback from users.</p><p>Parallel design is considered a user-centered design method, as it puts the user at the center of the design process, by involving them in the evaluation of multiple design options. It's useful when testing a wide range of design options, when the goal is to test usability, user flow, and overall user experience, and when the budget and time allows it.</p>Source: <a href="https://www.usabilitybok.org/parallel-design" target="_blank">usabilitybok.org</a>
		</method>

		<method id="functionallocation" category="formative,solutions,designprocess" title="Design Method: Function Allocation">
			<p>Function allocation is a method used in human-computer interaction (HCI) and systems engineering to determine the distribution of functions between human operators and computer systems. It is a technique that involves identifying the tasks and functions that are best suited to be performed by humans and the tasks and functions that are best suited to be performed by computer systems.</p><p>The process of function allocation typically involves identifying the functions and tasks that are required to complete a given task or process. These functions are then evaluated to determine which are best suited to be performed by humans and which are best suited to be performed by computer systems. Factors such as safety, efficiency, accuracy, and user preferences are considered when making these determinations.</p><p>Function allocation is important because it helps to ensure that the functions and tasks are performed by the most appropriate system, whether it is human or computer. This leads to more efficient, safe, and effective systems, as well as better user experiences.</p><p>Function allocation can be done using a variety of techniques, such as task analysis, functional analysis, and decision-making methods. The choice of technique will depend on the goals of the evaluation, the resources available, and the level of detail required.</p><p>Function allocation is essential for the design of systems that are safe, efficient, accurate, and easy to use. It's particularly useful in safety-critical systems such as aviation, nuclear power, and medical devices where the wrong allocation of functions could lead to severe consequences.</p>Source: <a href="https://www.usabilitybok.org/function-allocation" target="_blank">usabilitybok.org</a>
		</method>

		<method id="cardsorting" category="formative,solutions,designprocess" title="Design Method: Card Sorting">
			<p>Card sorting is a method used in human-computer interaction (HCI) and user-centered design to understand how users organize and categorize information. It is a technique that helps to identify the most natural and meaningful grouping of information for a particular user group.</p><p>The process of card sorting typically involves creating a set of cards, each representing a piece of information or content. These cards are then given to a group of users, who are asked to organize them into categories or groups that make sense to them. This can be done in a physical or digital form.</p><p>There are two main types of card sorting: open and closed. In open card sorting, users are free to create their own categories and labels, whereas in closed card sorting, users are asked to sort the cards into predefined categories or groups.</p><p>The output of card sorting is a visual representation of the categories and groups that the users have created, along with the labels or names that they have assigned to them. This information can be used to inform the design of a website, application, or other digital product, by providing insights into how users expect to find and access information.</p><p>Card sorting is a valuable tool for HCI designers because it allows them to understand how users naturally think about and organize information. It can help to identify patterns and connections in the data, and can inform decisions about the organization, labeling, and navigation of a digital product.</p>Source: <a href="https://en.wikipedia.org/wiki/Card_sorting" target="_blank">Wikipedia</a>
		</method>

		<method id="designthinking" category="formative,solutions,designprocess" title="Design Method: Design Thinking">
			<p>Design thinking is a problem-solving approach that is used in human-computer interaction (HCI) and user-centered design to understand and address the needs of users. It is a holistic and iterative process that involves empathizing with users, defining problems, ideating solutions, prototyping and testing. It is a human-centered approach that focuses on the needs and perspectives of users, rather than technical constraints or business goals.</p><p>The process of design thinking typically starts with understanding the needs and perspectives of users through techniques such as ethnographic research, interviews, and observations. Next, the problem is defined by synthesizing the research findings and identifying key issues and pain points. Then, a wide range of possible solutions are generated through brainstorming, mind-mapping, and other ideation techniques. After that, solutions are prototyped and tested with users to gather feedback and iterate on the design.</p><p>Design thinking is valuable in HCI and user-centered design because it helps to create products and services that are tailored to the needs and perspectives of users. It also allows for a more flexible and iterative design process, which leads to more innovative and effective solutions.</p><p>Design thinking is considered a user-centered design method, as it puts the user at the center of the design process. It is useful when the goal is to create products and services that are tailored to the needs and perspectives of users, when the goal is to generate new and innovative solutions, and when the goal is to create a flexible and iterative design process.</p>Source: <a href="https://en.wikipedia.org/wiki/Design_thinking" target="_blank">Wikipedia</a>
		</method>

		<method id="contextualinquiry" category="formative,solutions,designprocess" title="Design Method: Contextual Inquiry">
			<p>Contextual inquiry is a method used in human-computer interaction (HCI) to understand how users interact with technology in their natural environment. The goal of contextual inquiry is to understand the user's goals, tasks, workflows, and processes, as well as their physical and social context. The method of contextual inquiry typically involves conducting interviews with users in their work or home environment, observing them as they perform tasks using technology, and collecting artifacts such as notes, sketches, and photographs. The data collected during contextual inquiry is used to identify patterns and problems in the user's workflow and to understand how the technology fits into the user's broader context. Contextual inquiry is a valuable tool for HCI designers because it provides a more complete understanding of how users interact with technology and how it fits into their daily lives. This information can be used to inform the design of technology that is better suited to the user's needs and is more likely to be used effectively. Contextual inquiry is considered a qualitative research method, meaning the data collected is non-numerical, and it's analyzed by identifying patterns, themes, and categories in the data. It's useful when the research question is open-ended, and the aim is to understand the user's experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Contextual_inquiry" target="_blank">Wikipedia</a>
		</method>

		<method id="focusgroup" category="formative,solutions,designprocess" title="Design Method: Focus Group">
			<p>A focus group is a method used in human-computer interaction (HCI) and user-centered design to gather qualitative data about user opinions, attitudes, and behaviors. It is a technique that involves bringing a small, diverse group of people together in a moderated discussion to explore a specific topic or product.</p><p>The process of conducting a focus group typically involves recruiting a group of participants who match specific demographic or user criteria, such as age, gender, occupation, or experience with a specific product. The group then meets in a moderated discussion, led by a facilitator, to explore a specific topic or product. The discussion is usually recorded, either through audio or video, and the data collected is analyzed to identify patterns, themes, and key issues related to the topic or product.</p><p>Focus groups are useful in HCI and user-centered design because they allow researchers to gather rich, detailed data about user opinions, attitudes, and behaviors. They also allow researchers to explore a specific topic or product in-depth and to gain a deeper understanding of the user's perspective.</p><p>Focus groups are considered a qualitative research method, as they involve collecting and analyzing data in a non-numerical format. They're particularly useful when the goal is to understand users' attitudes, opinions and behaviors about a specific topic or product, when the goal is to generate new insights and ideas, and when the goal is to gather feedback on a specific product or design.</p>Source: <a href="https://en.wikipedia.org/wiki/Focus_group" target="_blank">Wikipedia</a>
		</method>

		<method id="storyboarding" category="formative,solutions,prototype" title="Design Method: Storyboarding">
			<p>Storyboarding is a method used in user-centered design and human-computer interaction (HCI) to visualize and communicate the design of an interface, product, or service. It is a technique that helps to plan and organize the user experience by creating a visual representation of the interactions and tasks that users will perform.</p><p>The process of storyboarding typically involves creating a series of drawings or illustrations that depict the different steps or screens in the user's journey. Each illustration is accompanied by notes or captions that describe the user's actions, thoughts, and feedback. The storyboard can be created using pencil and paper, digital tools, or a combination of both.</p><p>The output of storyboarding is a visual representation of the user experience that can be used to communicate the design to a wide range of stakeholders, including designers, developers, and users. It helps to identify potential issues or problems in the design and to make changes before the product or service is built.</p><p>Storyboarding is a valuable tool for HCI designers because it allows them to see the user's journey from start to finish and to identify potential issues or problems before they occur. It also helps to communicate the design to a wide range of stakeholders, including designers, developers, and users.</p><p>Storyboarding can also be used to create scenarios that capture the users' goals, needs, and expectations, and to evaluate the design against them. It's a powerful method to use when creating interactive products such as games, animations, and interactive web interfaces.</p>Source: <a href="https://en.wikipedia.org/wiki/Storyboard#Software" target="_blank">Wikipedia</a>
		</method>

		<method id="mockup" category="formative,solutions,prototype" title="Prototyping Method: Image/Video MockUp">
			<p>Image/Video mockup is a method used in human-computer interaction (HCI) and user-centered design to create a visual representation of a design that can be used to test and evaluate it with users. It is a technique that involves creating a realistic image or video of the design, which can be used to simulate the look and feel of the final product.</p><p>The process of creating an image/video mockup typically involves using digital tools such as graphic design software, or specialized prototyping tools. The mockup is created to look as realistic as possible, and includes visual design elements such as color, typography, images, and even animations.</p><p>During the testing, users interact with the mockup, and the designer or facilitator can observe and note any issues or problems that arise. The mockup can be used to test the overall visual design, branding, and user experience.</p><p>Image/video mockup is a valuable tool for HCI designers because it allows them to test and evaluate the visual design of a product or service early in the design process, without the need for expensive or time-consuming software development. It also allows designers to make changes and iterate on the design quickly, based on feedback from users.</p><p>Image/video mockup is considered a high-fidelity prototyping method, as it's a realistic representation of the design, and it includes visual design elements. It's useful when testing mid or late-stage designs, when the goal is to test the overall visual design, branding, and user experience, and when the budget allows for it.</p>Source: <a href="https://en.wikipedia.org/wiki/Mockup" target="_blank">Wikipedia</a>
		</method>

		<method id="paperprototype" category="formative,solutions,prototype" title="Prototyping Method: Paper Prototype">
			<p>Paper prototyping is a method used in human-computer interaction (HCI) and user-centered design to test and evaluate designs with users. It is a technique that involves creating a rough, hand-drawn or printed representation of the design, which can be used to simulate interactions and test basic usability.</p><p>The process of paper prototyping typically involves creating a series of sketches or illustrations that depict the different screens or interactions of the design. These sketches can be created using pencil and paper, or printed from a digital tool. The sketches are then cut out and arranged on a board or table to simulate the different screens or interactions.</p><p>During the testing, a facilitator or designer acts as the system, and moves the paper sketches in response to the user's actions and input. This allows the user to interact with the design, and the designer to observe and note any issues or problems that arise.</p><p>Paper prototyping is a valuable tool for HCI designers because it is quick and easy to create, and it allows them to test basic usability and interactions early in the design process. It also allows designers to make changes and iterate on the design quickly, without the need for expensive or time-consuming software development.</p><p>Paper prototyping is considered a low-fidelity prototyping method, as it's a rough representation of the design, and the user's interactions are simulated rather than real. It's useful when testing early stage designs and when the goal is to test usability, user flow, and overall user experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Paper_prototyping" target="_blank">Wikipedia</a>
		</method>

		<method id="rapidprototype" category="formative,solutions,prototype" title="Prototyping Method: Rapid Prototype">
			<p>Rapid prototyping is a method used in human-computer interaction (HCI) and user-centered design to quickly create a working model of a design, which can be tested and evaluated with users. The goal of rapid prototyping is to create a functional prototype of the design as quickly as possible, in order to test and evaluate it with users.</p><p>The process of rapid prototyping typically involves creating a prototype using simple and readily available materials, such as paper, cardboard, or foam board. The prototype is usually created quickly, and the focus is on functionality rather than aesthetics.</p><p>Rapid prototyping can be done using a variety of techniques, such as paper prototyping, wireframing, or 3D printing. The choice of technique will depend on the goals of the evaluation, the resources available, and the level of detail required.</p><p>During the testing, users interact with the prototype, and the designer or facilitator can observe and note any issues or problems that arise. The prototype can be used to test the functionality, usability, and overall user experience.</p><p>Rapid prototyping is a valuable tool for HCI designers because it allows them to test and evaluate the design early in the design process, without the need for expensive or time-consuming software development. It also allows designers to make changes and iterate on the design quickly, based on feedback from users.</p><p>Rapid prototyping is considered a low-fidelity prototyping method, as the prototype is usually created quickly, and the focus is on functionality rather than aesthetics. It's useful when testing early stage designs, when the goal is to test functionality, usability, and overall user experience, and when the budget or time is limited.</p>Source: <a href="https://en.wikipedia.org/wiki/Rapid_prototyping" target="_blank">Wikipedia</a>
		</method>

		<method id="wireframe" category="formative,solutions,prototype" title="Prototyping Method: Wireframe">
			<p>Wireframes are a method used in human-computer interaction (HCI) and user-centered design to test and evaluate designs with users. It is a technique that involves creating a simple, skeletal representation of the design, which can be used to test layout, navigation, and functionality.</p><p>A wireframe is a visual representation of the layout and structure of a website, application, or other digital product, without the use of visual design elements such as color, typography, or images. They are used to represent the interface and its layout, the organization of elements, and the relationships between them.</p><p>The process of creating wireframes typically involves using a digital tool such as a wireframing software, or just using pen and paper. The wireframe is created using simple shapes and lines, and includes the placement of elements such as buttons, text, images, and navigation elements.</p><p>During the testing, users interact with the wireframe, and the designer or facilitator can observe and note any issues or problems that arise. The wireframe can be used to test layout, navigation, and functionality, and to identify potential issues with the design before it is built.</p><p>Wireframes are a valuable tool for HCI designers because they allow them to test and evaluate the layout, navigation, and functionality of a design early in the design process. They also allow designers to make changes and iterate on the design quickly, without the need for expensive or time-consuming software development.</p><p>Wireframes are considered a low-fidelity prototyping method, as they lack visual design elements and details, and the user's interactions are simulated rather than real. They're useful when testing early stage designs, when the goal is to test layout, navigation, and functionality, and when the budget or time is limited.</p>
 Source: <a href="https://en.wikipedia.org/wiki/Website_wireframe" target="_blank">Wikipedia</a>
		</method>

		<method id="storyboard" category="formative,solutions,prototype" title="Prototyping Method: Storyboard">
			<p>A storyboard prototype is a method used in human-computer interaction (HCI) and user-centered design to create a visual representation of a product or service's user experience. It is a technique that involves creating a visual representation of the user's journey through the product or service in the form of a storyboard. A storyboard is a sequence of illustrations or images that depict the key steps or actions that a user takes when interacting with the product or service.</p><p>The process of creating a storyboard prototype typically involves identifying the key steps or actions that a user takes when interacting with the product or service, and creating illustrations or images that depict those steps or actions. The storyboard is then used to test and evaluate the user experience with users, and to identify any issues or problems that arise.</p><p>Storyboard prototypes are useful in HCI and user-centered design because they allow designers to create a visual representation of the user experience and test it with users early in the design process. They also allow designers to make changes and iterate on the design quickly, based on feedback from users.</p><p>Storyboard prototypes are considered a low-fidelity prototyping method, as they are a simplified representation of the design and do not include all the visual design elements. They are useful when testing early stage designs, when the goal is to test the overall user flow and user experience, and when the budget and time are limited.</p> Source: <a href="https://en.wikipedia.org/wiki/Storyboard#Software" target="_blank">Wikipedia</a>
		</method>

		<method id="summativeusabilitytesting" category="summative,empirical,problems" title="Usability Inspection by Users: Summative Usability Testing">
			<p>Summative usability testing is a method used in human-computer interaction (HCI) and user-centered design to evaluate the usability and user experience of a product or service. It is a technique that involves testing the product or service with a representative sample of users to gather data on its usability and user experience.</p><p>The process of summative usability testing typically involves recruiting a representative sample of users who match specific demographic or user criteria, such as age, gender, occupation, or experience with a specific product. The users then perform a set of tasks or scenarios while their behavior is observed and recorded. The data collected is analyzed to identify patterns, themes, and key issues related to the usability and user experience of the product or service.</p><p>Summative usability testing is valuable in HCI and user-centered design because it allows researchers to gather data on the usability and user experience of a product or service, and to identify any issues or problems that arise. It also allows researchers to make data-driven decisions about the design of the product or service.</p><p>Summative usability testing is considered a quantitative research method, as it involves collecting and analyzing numerical data. It's particularly useful when the goal is to evaluate the usability and user experience of a final product or service, when the goal is to identify specific issues or problems with a product or service, and when the goal is to gather data to support design decisions.</p> Source: <a href="https://www.usabilitybok.org/summative-usability-testing" target="_blank">usabilitybok.org</a>
		</method>

		<method id="benchmarktesting" category="formative,empirical,problems,experts" title="Usability Inspection by Users: Benchmark Testing">
			<p>Benchmark Testing is a method used to evaluate the usability of digital products or systems by having users perform tasks and then comparing their performance to a benchmark or standard.</p><p>In benchmark testing, a group of participants are asked to complete a set of tasks while interacting with a product or system. The performance of these participants is then compared to a benchmark, which can be an established standard or the performance of a similar group of participants on a similar product or system.</p><p>The benchmark can be established by performing a pilot study, where a group of participants complete the tasks and their performance is used as the benchmark. The results of the pilot study are used to establish a standard for future testing.</p><p>The main goal of benchmark testing is to identify areas for improvement and to compare the usability of different products or systems. The method is useful in identifying usability issues that might be overlooked in other methods such as surveys and interviews.</p><p>It is important to consider that benchmark testing is a comparative method, and it is not a standalone method for evaluating the usability of a product or system. Benchmark testing should be used in conjunction with other methods such as usability testing, expert reviews, and user interviews, to gain a comprehensive understanding of the user's experience.</p>Source: <a href="https://about.gitlab.com/handbook/engineering/ux/ux-research-training/usability-benchmarking" target="_blank">gitlab.com</a>
		</method>

		<method id="remoteevaluation" category="formative,empirical,problems,experts" title="Usability Inspection by Users: Remote Evaluation">
			<p>Remote Evaluation is a method used in Human-Computer Interaction (HCI) to evaluate the usability of digital products or systems remotely. This method allows researchers to conduct evaluations remotely, rather than in a lab setting. Remote evaluations are conducted over the internet, and can be done from anywhere and at any time, making it possible to test products or systems with users in different locations and time zones.</p><p>Remote evaluations can be done in different ways, such as having participants complete a set of tasks while interacting with a product or system, and then providing feedback on their experience. The feedback can be in the form of written comments, video recordings, or audio recordings.</p><p>Remote evaluations have several advantages over traditional in-person evaluations. They allow researchers to test products or systems with a larger and more diverse group of users, and also make it possible to test products or systems in a more natural setting. Remote evaluations are also more cost-effective and time-efficient, as they do not require participants to travel to a lab or testing facility.</p><p>However, it's important to note that remote evaluations have some limitations, such as the lack of control over the testing environment and the potential technical difficulties. Additionally, remote evaluations may not provide the same level of detail as in-person evaluations, as researchers do not have direct access to the user's environment or body language. Therefore, it's important to use multiple methods to gain a comprehensive understanding of user's experience.</p>Source: <a href="https://www.usabilitybok.org/remote-evaluation" target="_blank">usabilitybok.org</a>
		</method>

		<method id="heuristicevaluation" category="formative,empirical,problems,experts" title="Usability Inspection by Experts: Heuristic Evaluation">
			<p>Heuristic Evaluation is a method used in Human-Computer Interaction (HCI) to evaluate the usability of digital products or systems. It is a formal method for identifying usability problems by having a group of experts evaluate the product or system against a set of established usability principles, known as heuristics.</p><p>In Heuristic Evaluation, a group of experts, such as usability specialists, interaction designers, or user experience designers, are asked to evaluate a product or system against a set of established heuristics. These heuristics are a set of general principles for good design, such as consistency, visibility of system status, and error prevention.</p><p>The experts are asked to identify any usability issues that they find while using the product or system, and to provide detailed descriptions of the issues and how they relate to the heuristics. The experts then meet to discuss the issues and to prioritize them based on their severity and impact on the user.</p><p>Heuristic Evaluation is a valuable method in HCI research, as it allows researchers to identify usability problems early in the design process, before they become more costly and time-consuming to fix. It is also cost-effective, as it only requires a small number of experts to evaluate the product or system. However, it's important to note that Heuristic Evaluation is a comparative method, and it is not a standalone method for evaluating the usability of a product or system. Heuristic Evaluation should be used in conjunction with other methods such as usability testing, expert reviews, and user interviews, to gain a comprehensive understanding of the user's experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Heuristic_evaluation" target="_blank">Wikipedia</a>
		</method>

		<method id="consistencyinspection" category="empirical,problems,experts" title="Usability Inspection by Experts: Consistency Inspection">
			<p>Consistency Inspection is a method used to evaluate the consistency of a digital product or system. Consistency refers to the degree to which the product or system's user interface elements, such as layout, wording, icons, and interactions, are consistent throughout the product. Consistency is important because it helps users to understand and predict the behavior of the system, and reduces the cognitive load on the user.</p><p>Consistency Inspection is usually done by having a group of experts, such as usability specialists, interaction designers, or user experience designers, evaluate the product or system for consistency. The experts are asked to identify any inconsistencies they find and to provide detailed descriptions of the issues.</p><p>During the inspection, experts are looking for things like consistent use of layout, wording, icons, and interactions across all the pages, or screens of the product, as well as consistency in the use of error messages, feedback, and help functions.</p><p>Consistency Inspection is a valuable method in HCI research, as it allows researchers to identify inconsistencies early in the design process, before they become more costly and time-consuming to fix. It is also cost-effective, as it only requires a small number of experts to evaluate the product or system. However, it's important to note that Consistency Inspection is a comparative method, and it is not a standalone method for evaluating the consistency of a product or system. Consistency Inspection should be used in conjunction with other methods such as usability testing, expert reviews, and user interviews, to gain a comprehensive understanding of the user's experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Usability#Inspection_methods" target="_blank">Wikipedia</a>
		</method>

		<method id="pluralisticinspection" category="formative,empirical,problems,experts" title="Usability Inspection by Experts: Pluralistic inspection">
			<p>Pluralistic Inspection is a method used to evaluate the usability of a digital product or system by combining the results from multiple inspection methods. It can involve a combination of different inspection methods, such as Heuristic Evaluation, Cognitive Walkthrough, and Consistency Inspection, to identify usability problems.</p><p>The idea behind Pluralistic Inspection is that different inspection methods have different strengths and weaknesses, and by using multiple methods, researchers can gain a more comprehensive understanding of the usability issues. For example, Heuristic Evaluation can identify usability problems early in the design process, while usability testing can identify issues that arise during real-world use.</p><p>During Pluralistic Inspection, each inspection method is applied to the product or system, and the results are then combined and compared to identify common issues and prioritize them based on their severity and impact on the user.</p><p>Pluralistic Inspection is a valuable method in HCI research, as it allows researchers to identify usability problems early in the design process, before they become more costly and time-consuming to fix. It also provides a more comprehensive understanding of the user's experience by combining the results from multiple methods. However, it is important to keep in mind that Pluralistic Inspection can be time-consuming and resource-intensive, as it requires the use of multiple methods.</p>Source: <a href="https://en.wikipedia.org/wiki/Usability#Inspection_methods" target="_blank">Wikipedia</a>
		</method>

		<method id="activityanalysis" category="formative,empirical,problems,experts" title="Usability Inspection by Experts: Activity Analysis">
			<p>Activity Analysis is a method used to understand how experts interact with digital products or systems, and to identify opportunities for improvement. It is a method used to study the activities, tasks, and processes that experts perform while using a product or system.</p><p>Activity Analysis involves observing experts as they perform tasks and use a product or system, and then analyzing their actions, behaviors, and interactions. This can be done through methods such as observation, interviews, and diary studies.</p><p>During the analysis, researchers look for patterns and trends in the data, and use this information to identify opportunities for improvement. For example, they might identify areas where users are struggling, or where the system could be made more efficient.</p><p>Activity Analysis is a valuable method in HCI research, as it provides insight into how people actually use a product or system, rather than just what they say they do. It can be used to identify usability problems and to make design recommendations to improve the user experience. It is also useful in identifying unanticipated use cases and new opportunities for a product or system. It can be used in combination with other methods such as usability testing, expert reviews, and user interviews, to gain a comprehensive understanding of the user's experience.</p>Source: <a href="https://en.wikipedia.org/wiki/Usability#Inspection_methods" target="_blank">Wikipedia</a>
		</method>

		<method id="taskanalysis" category="formative,analytical,datadriven" title="Data-driven Analysis: Cognitive/Hierarchical Task Analysis (CTA/HTA)">
			<p>Cognitive Task Analysis (CTA) and Hierarchical Task Analysis (HTA) are data-driven methods for understanding how people perform tasks and interact with digital products or systems. They are used to identify the mental processes and strategies that people use to complete tasks, as well as the structure and organization of tasks.</p><p>CTA is a method that allows researchers to understand the mental processes and strategies that people use to complete tasks. It typically involves collecting data through methods such as observation, interviews, and think-aloud protocols. The data is analyzed to identify the specific mental processes and strategies used by the users, such as problem-solving, decision-making, and memory.</p><p>HTA, on the other hand, is a method that helps researchers to understand the structure and organization of tasks. It involves breaking down tasks into smaller and more manageable subtasks and then organizing these subtasks into a hierarchical structure. This hierarchical structure shows the relationship between subtasks and the order in which they need to be completed. This data is collected through methods such as observation, interviews, and task decomposition.</p><p>Both CTA and HTA are valuable methods in HCI research, as they provide a comprehensive understanding of how people interact with digital products or systems. By identifying mental processes, strategies, and task structure, researchers can identify areas for improvement and make design recommendations to enhance the user experience.</p>Source: <a href="https://www.usabilitybok.org/hierarchical-task-analysis" target="_blank">usabilitybok.org</a>
		</method>

		<method id="groundedtheory" category="formative,analytical,datadriven" title="Data-driven Analysis: Grounded Theory">
			<p>Grounded Theory is a research method used in Human-Computer Interaction (HCI) to generate a theoretical understanding of a phenomenon through the analysis of data. It is an inductive method that allows researchers to generate theory from data, rather than testing a preconceived theory.</p><p>Grounded Theory is typically used in qualitative research and it involves collecting and analyzing data, such as interviews, observations, or written documents, to identify patterns, themes, and relationships. The data is analyzed repeatedly to identify emerging patterns, and these patterns are used to generate a theoretical framework that explains the phenomenon being studied.</p><p>The coding process typically includes the following steps:</p><ol><li>Open coding: This is the initial stage of coding, where the data is read and broken down into small chunks of information, called "codes." Codes are assigned to specific segments of the data, based on the content and meaning.</li><li>Axial coding: This stage involves grouping the codes into broader categories, based on similarities and relationships.</li><li>Selective coding: This stage involves identifying the core category, which is the central concept that connects all the other categories.</li><li>Memo writing: Memos are written to document the researcher's thoughts and observations about the data and the emerging categories.</li><li>Constant comparison: This is an ongoing process where new data is compared to the existing codes and categories to identify similarities and differences, and to refine the coding scheme as needed.</li></ol><p>These coding steps are iterative and cyclical, meaning that the researcher will repeatedly go back and forth between the different coding steps to ensure that the codes, categories, and theoretical framework accurately reflect the data.</p><p>The goal of coding is to identify patterns and relationships in the data and to generate a theoretical framework that explains the phenomenon being studied. Coding is an essential step in Grounded Theory, as it allows researchers to make sense of the data and to generate a comprehensive theoretical framework.</p><p>Grounded Theory is a valuable method in HCI research as it allows researchers to generate theory from data, rather than testing a preconceived theory. It also allows researchers to identify patterns and relationships that might not be obvious from the data alone. It's also useful in identifying new opportunities and unanticipated use cases. However, it's important to keep in mind that Grounded Theory is a time-consuming method and it requires a significant amount of data to generate a comprehensive theoretical framework.</p>Source: <a href="https://en.wikipedia.org/wiki/Grounded_theory" target="_blank">Wikipedia</a>
		</method>

		<method id="personas" category="formative,analytical,datadriven" title="Data-driven Analysis: Personas">
			<p>Personas, or fictional characters based on user research and feedback, have revolutionized Human-Computer Interaction and created a personalized experience for users. In today's world, technology plays a prevalent role in everyday life, and it is increasingly important to ensure that users have an efficient and enjoyable experience when interacting with such technology. This essay will discuss the concept of personas and their role in human-computer interaction, highlighting how their implementation has allowed users to have a tailored experience.</p><p>Personas are especially useful for uncovering the nuances of the peopleâ€™s characteristics and requirements. Through user research and personas, designers can create products and services that are tailored to the needs, while also taking into consideration their physical, cognitive, and emotional capacities.</p><p>Data-Driven Personas (DDP) are an increasingly popular human-computer interaction (HCI) technique for understanding user behavior. According to Jansen, Salminen, Jung, and Guan (2021), DDPs â€œare generated from user data such as demographics, interests, behavior, and preferencesâ€ and are used to â€œmake more informed product decisionsâ€. As the authors note, DDPs are a powerful tool for understanding user behavior and providing tailored experiences that meet user needs. Furthermore, they can be used alongside other HCI techniques such as user research and interviews to further understand user behavior. By combining information gathered through user research and interviews with insights gained through DDPs, designers and developers can create user experiences that are tailored to the specific needs of their users. Additionally, DDPs can be used to identify and address areas of user dissatisfaction that may not be picked up by other HCI techniques. In conclusion, DDPs are an effective tool for understanding user behavior and should be used alongside other HCI techniques to create user experiences that are tailored to the specific needs of the users.</p>
			Source: <a href="https://www.usabilitybok.org/persona" target='_blank'>Wikipedia</a>, <i>BJ Jansen. , J Salminen. , S Jung. , K Guan. Using Data-Driven Personas Alongside Other Human-Computer Interaction (HCI) Techniques.<a href="https://link.springer.com/chapter/10.1007/978-3-031-02231-9_8" target='_blank'>Springer</a></i>
		</method>

		<method id="fittsmodel" category="analytical,modeldriven" title="Model-driven Analysis: Fitts' Law">
			<p>Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humanâ€“computer interaction and ergonomics. Over the years, different variations of the original law have been used in various HCI projects to further study the behavior of the user and design better user experiences. In this essay, I will discuss the various aspects of Fitts Law and explain how it has been utilized in the field of HCI such as for 2D or 3D.<p></p>The law predicts that the time required to rapidly move to a target area is a function of the ratio between the distance to the target and the width of the target. For a model-driven analysis a device's constants (a and b) in his formula [1] must be known.<p></p>Fitts's law is used to model the act of pointing, either by physically touching an object with a hand or finger, or virtually, by pointing to an object on a computer monitor using a pointing device. Furthermore, it has been used to investigate how users interact with touchscreens, such as those found in mobile phones and tablet computers. As such, Fitts law is an important research and design tool that can be used to improve the user experience of HCI.</p> Source: <a href="https://en.wikipedia.org/wiki/Fitts%27s_law" target='_blank'>Wikipedia</a>
		</method>

		<method id="goms" category="analytical,modeldriven" title="Model-driven Analysis: Goals, Operators, Methods, and Selection (GOMS)">
			GOMS, or Goals, Operators, Methods, and Selection, is an important and popular model for analyzing user interfaces in human-computer interaction. GOMS is effective when it comes to studying complex .Source: <a href="https://en.wikipedia.org/wiki/CPM-GOMS" target='_blank'>Wikipedia</a>
		</method>

		<method id="cpmgoms" category="analytical,modeldriven" title="Model-driven Analysis: Cognitive Perceptual Motor GOMS (CPM-GOMS)">
			GOMS is a specialized human information processor model for human-computer interaction observation that describes a user's cognitive structure on four components. In the book The Psychology of Human Computer Interaction. written in 1983 by Stuart K. Card, Thomas P. Moran and Allen Newell, the authors introduce: "a set of Goals, a set of Operators, a set of Methods for achieving the goals, and a set of Selections rules for choosing among competing methods for goals." GOMS is a widely used method by usability specialists for computer system designers because it produces quantitative and qualitative predictions of how people will use a proposed system. Source: <a href="https://en.wikipedia.org/wiki/GOMS" target='_blank'>Wikipedia</a>
		</method>

		<method id="cmngoms" category="analytical,modeldriven" title="Model-driven Analysis: Card, Moran and Newell GOMS (CMN-GOMS)">
			CMN-GOMS is the original GOMS model proposed by Stuart Card, Thomas P. Moran and Allen Newell. CMN stands for Card, Moran and Newell and it takes the KLM as its basic and adds subgoals and selection rules. This model can predict operator sequence as well as execution time. A CMN-GOMS model can be represented in program form, making it amenable to analysis as well as execution. CMN-GOMS has been used to model word processors and CAD systems for ergonomic design(see CAD). The CMN method can predict the operator sequence and the execution time of a task on a quantitative level and can focus its attention on methods to accomplish goals on a qualitative level. Source: <a href="https://en.wikipedia.org/wiki/GOMS#CMN-GOMS" target='_blank'>Wikipedia</a>
		</method>

		<method id="ngomsl" category="analytical,modeldriven" title="Model-driven Analysis: Structured Natural Language GOMS (CMN-GOMS)">
			NGOMSL is a structured natural language notation for representing GOMS models and a procedure for constructing them. This program form provides predictions of operator sequences, execution time and time to learn methods. An analyst constructs an NGOMSL model by performing a top-down, breadth-first expansion of the user's top-level goals into methods, until the methods contain only primitive operators, typically keystroke-level operators. This model explicitly represents the goal structure just like the CMN-GOMS and can so represent high-level goals. Source: <a href="https://en.wikipedia.org/wiki/GOMS#NGOMSL" target='_blank'>Wikipedia</a>
		</method>

		<method id="sgoms" category="analytical,modeldriven" title="Model-driven Analysis: Sociotechnical GOMS (CMN-GOMS)">
			SGOMS stands for Sociotechnical GOMS and was created to allow GOMS to model work in complex sociotechnical systems. GOMS is meant to model an individual user, working in isolation, with no unexpected interruptions, similar to a Cognitive Psychology experiment. This level of analysis is sometimes referred to as microcognition to distinguish it from macrocognition, which refers to real world cognition. SGOMS is meant to expand the applicability of GOMS to the macro cognitive level of analysis. To do this, SGOMS adds a high level control structure to GOMS, called the planning unit. This allows GOMS to deal with unexpected interruptions. Source: <a href="https://en.wikipedia.org/wiki/GOMS#SGOMS" target='_blank'>Wikipedia</a>
		</method>

		<method id="klm" category="analytical,modeldriven" title="Model-driven Analysis: Keystroke-level Model (KLM)">
			In humanâ€“computer interaction, the keystroke-level model (KLM) predicts how long it will take an expert user to accomplish a routine task without errors using an interactive computer system. It was proposed by Stuart K. Card, Thomas P. Moran and Allen Newell in 1980 in the Communications of the ACM and published in their book The Psychology of Human-Computer Interaction in 1983, which is considered as a classic in the HCI field. The foundations were laid in 1974, when Card and Moran joined the Palo Alto Research Center (PARC) and created a group named Applied Information-Processing Psychology Project (AIP) with Newell as a consultant aiming to create an applied psychology of human-computer interaction. The keystroke-level model is still relevant today, which is shown by the recent research about mobile phones and touchscreens (see Adaptions) Source: <a href="https://en.wikipedia.org/wiki/Keystroke-level_model" target='_blank'>Wikipedia</a>
		</method>

		<method id="cognitivewalkthrough" category="formative,empirical,problems,experts" title="Usability Inspection by Experts: Cognitive walkthrough">
			The cognitive walkthrough method is a usability inspection method used to identify usability issues in interactive systems, focusing on how easy it is for new users to accomplish tasks with the system. A cognitive walkthrough is task-specific, whereas heuristic evaluation takes a holistic view to catch problems not caught by this and other usability inspection methods. The method is rooted in the notion that users typically prefer to learn a system by using it to accomplish tasks, rather than, for example, studying a manual. The method is prized for its ability to generate results quickly with low cost, especially when compared to usability testing, as well as the ability to apply the method early in the design phases before coding even begins (which happens less often with usability testing). Source: <a href="https://en.wikipedia.org/wiki/Cognitive_walkthrough" target='_blank'>Wikipedia</a>
		</method>

		<method id="eyetracking" category="empirical,quantitative,objective,attention" title="Quantitative Objective Attention: Eye Tracking Fixations">
			Eye tracking is the process of measuring either the point of gaze (where one is looking) or the motion of an eye relative to the head. An eye tracker is a device for measuring eye positions and eye movement. Eye trackers are used in research on the visual system, in psychology, in psycholinguistics, marketing, as an input device for human-computer interaction, and in product design. Eye movements are typically divided into <strong>fixations</strong> and <strong>saccades</strong> - when the eye gaze pauses in a certain position, and when it moves to another position, respectively. The resulting series of fixations and saccades is called a <strong>scanpath</strong>. Smooth pursuit describes the eye following a moving object. Fixational eye movements include microsaccades: small, involuntary saccades that occur during attempted fixation. Most information from the eye is made available during a fixation or smooth pursuit, but not during a saccade.
			<p class="mt-2">Eye tracking requires special devices: Eye trackers.</p>
			<p class="mt-2">Source: </p>
			Holmqvist, K., NystrÃ¶m, M., Andersson, R., Dewhurst, R., Jarodzka, H., and Van de Weijer, J. (2011). Eye tracking: A comprehensive guide to methods and measures. OUP Oxford.
		</method>

		<method id="rula" category="empirical,quantitative,objective,ergonomics" title="Quantitative Objective Ergonomics: Rapid Upper Limb Assessment (RULA)">
		</method>

		<method id="reba" category="empirical,quantitative,objective,ergonomics" title="Quantitative Objective Ergonomics: Rapid Entire Body Assessment (REBA)">
		</method>

		<method id="wearscale" category="empirical,quantitative,subjective,socialacceptance" title="Quantitative Subjective Social Acceptance: Wearable Acceptability Range (WEAR Scale v.3) ">
		</method>

		<method id="uvq" category="empirical,quantitative,subjective,humanlikeness" title="Quantitative Subjective Human-Likeness: Uncanny Valley Questionnaire (UVQ) ">
		</method>

		<method id="scm" category="empirical,quantitative,subjective,socialacceptance" title="Quantitative Subjective Social Acceptance: Stereotype Content Model (SCM) ">
		</method>

		<method id="mq" category="empirical,quantitative,subjective,motivation" title="Quantitative Subjective Motivation: Motivation Questionnaire (MQ) ">
		</method>

		<method id="imi" category="empirical,quantitative,subjective,motivation" title="Quantitative Subjective Motivation: Intrinsic Motivation Inventory (IMI) ">
		</method>

		<method id="vbo" category="empirical,quantitative,subjective,bodyownership" title="Quantitative Subjective Body-Ownership: Virtual-Body Ownership (VBO)">
		</method>

		<method id="vhi" category="empirical,quantitative,subjective,bodyownership" title="Quantitative Subjective Hand-Ownership: Virtual-Hand Illusion (VHI)">
		</method>

		<method id="ieq" category="empirical,quantitative,subjective,immersion,games" title="Quantitative Subjective Immersion: Immersive Experience Questionnaire (IEQ)">
		</method>

		<method id="ari" category="empirical,quantitative,subjective,immersion" title="Quantitative Subjective Immersion: Augmented Reality Immersion (ARI)">
		</method>

		<method id="wspq" category="empirical,quantitative,subjective,presence" title="Quantitative Subjective Presence: Wittmer-Singer Presence Questionnaire  (PQ)">
		</method>

		<method id="ipq" category="empirical,quantitative,subjective,presence" title="Quantitative Subjective Presence: i-group presence questionnaire  (IPQ)">
			The iGroup Presence Questionnaire (IPQ) is a 14-item scale for measuring the sense of presence experienced in a virtual environment (VE). It has been constructed using a large pool of items and two survey waves with approximately 500 participants. The scales has 4 subscales:
			<div class="list-group">
				<div class="list-group-item"><div><div class="fw-bold">General Presence</div> <div></div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Spatial Presence</div><div></div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Involvement</div><div></div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Realism</div><div></div></div></div>
			</div>
			<p class="mt-2"><strong><a href='http://www.igroup.org/pq/ipq/download.php' target='_blank'>Download Items</a></strong></p>
			Sources:
			Regenbrecht, H., and Schubert, T. (2002). Real and illusory interaction enhance presence in virtual environments. Presence: Teleoperators and Virtual Environments, 11(4), 425-434.
		</method>

		<method id="attrakdiffmini" category="empirical,quantitative,subjective,usability" title="Quantitative Subjective Usability: Pragmatic and Hedonic Qualitites (AttrakDiff Mini)">
		</method>

		<method id="attrakdiff" category="empirical,quantitative,subjective,usability" title="Quantitative Subjective Usability: Pragmatic and Hedonic Qualitites (AttrakDiff)">
		</method>

		<method id="umux" category="empirical,quantitative,subjective,usability" title="Quantitative Subjective Usability: Usability Metric for User Experience (UMUX)">
		</method>

		<method id="usabilitysus" category="empirical,quantitative,subjective,usability" title="Quantitative Subjective Usability: System Usability Scale (SUS)">
			The System Usability Scale (SUS) provides a â€œquick and dirtyâ€, reliable tool for measuring the usability.It consists of a 10 item questionnaire with five response options for respondents; from Strongly agree to Strongly disagree.  Originally created by John Brooke in 1986, it allows you to evaluate a wide variety of products and services, including hardware, software, mobile devices, websites and applications.
			<p class="mt-2"><a href="https://en.wikipedia.org/wiki/System_usability_scale" target='_blank'>Wikipedia</a></p>
			<p><a href="https://stuart-cunningham.github.io/sus/" target='_blank'>SUS Score Calculator</a>
			</p>
			<p>Brooke, J. (1996). "SUS: a "quick and dirty" usability scale". In P. W. Jordan; B. Thomas; B. A. Weerdmeester; A. L. McClelland (eds.). Usability Evaluation in Industry. London: Taylor and Francis.</p>
		</method>

		<method id="nasatlx" category="empirical,quantitative,subjective,workload,randomsample" title="Quantitative Subjective Workload: NASA Taskload Index Questionnaire (NASA-TLX)">
			The NASA Task Load Index (NASA-TLX) is a widely used, subjective, multidimensional assessment tool that rates perceived workload in order to assess a task, system, or team's effectiveness or other aspects of performance. There is a weighted and raw version:
			<div class="list-group">
				<div class="list-group-item"><div><div class="fw-bold">Raw TLX</div><div>(just the raw scales)</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Weighted TLX</div><div>(weighting and factor multiplication with the invidiual scales)</div></div></div>
			</div>
			<p class="mt-2">The total workload is divided into six subjective subscales: </p>
			<div class="list-group">
				<div class="list-group-item"><div><div class="fw-bold">Mental Demand</div> <div>How much mental and perceptual activity was required? </div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Physical Demand</div><div>How much physical activity was required?</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Temporal Demand</div><div>How much time pressure did you feel due to the pace at which the tasks or task elements occurred?</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Performance</div><div>How successful were you in performing the task?</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Effort</div><div>How hard did you have to work (mentally and physically) to accomplish your level of performance?</div></div></div>
				<div class="list-group-item"><div><div class="fw-bold">Frustration</div><div>How irritated, stressed, and annoyed versus content, relaxed, and complacent did you feel during the task?</div></div></div>
			</div>
			<p>NASA-TLX Downloads: <a href="https://humansystems.arc.nasa.gov/groups/tlx/downloads/TLXScale.pdf">PDF</a>, <a href="https://www.keithv.com/software/nasatlx/">Online Version</a>, <a href="https://humansystems.arc.nasa.gov/groups/tlx/tlxapp.php">Apps for iOS and Android</a>
			</p>
			<strong><a href='https://en.wikipedia.org/wiki/NASA-TLX' target='_blank'>Wikipedia</a></strong>
			<strong>Original Work</strong>: Hart, Sandra G.; Staveland, Lowell E. (1988). <a href="http://usd-apps.usd.edu/coglab/schieber/psyc792/workload/Hart-Staveland-1988.pdf">Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research" </a>. In Hancock, Peter A.; Meshkati, Najmedin (eds.). Human Mental Workload. Advances in Psychology. Vol. 52. Amsterdam: North Holland. pp. 139â€“183. doi:10.1016/S0166-4115(08)62386-9. ISBN 978-0-444-70388-0.
			<strong>Follow Up Work</strong>: Hart, Sandra G. (October 2006). <a href='https://humansystems.arc.nasa.gov/groups/TLX/downloads/HFES_2006_Paper.pdf' target='_blank'>NASA-task load index (NASA-TLX); 20 years later.</a> In Proceedings of the human factors and ergonomics society annual meeting (Vol. 50, No. 9, pp. 904-908). Sage CA: Los Angeles, CA: Sage publications.
		</method>

		<method id="geq" category="empirical,quantitative,objective,games,randomsample" title="Quantitative Subjective Gaming Experience: Gaming Experience Questionnaire (GEQ)">
			The Game Experience Questionnaire has a modular structure and consists of : 1. The core questionnaire, 2. The Social Presence Module, 3. The Post-game module., In addition to these modules, a concise in-game version of the GEQ was developed. All three modules are meant to be administered immediately after the game-session has finished, in the order given above. Part one and two probe the playersâ€™ feelings and thoughts while playing the game; Part 3, the post-game module, assesses how players felt after they had stopped playing.
			<p>IJsselsteijn, W. A., De Kort, Y. A., and Poels, K. (2013). <a href="https://research.tue.nl/en/publications/the-game-experience-questionnaire" target='_blank'>The game experience questionnaire.</a></p>
		</method>

		<method id="errorrate" category="empirical,quantitative,objective,performance,randomsample" title="Quantitative Objective Performance: Input Error Rate (%)">
			It is the ratio of the number of erroneous units of data to the total number of units of data transmitted.
		</method>

		<method id="accuarcy" category="empirical,quantitative,objective,performance,randomsample" title="Quantitative Objective Performance: Pointing Accuracy (Target Offset)">
			Pointing Accuracy measures the uncertainty of the pointing of an instrument along a selected direction based on vector measurements to a set of known beacons, e.g. stars, ground or space objects.
		</method>

		<method id="textentry" category="empirical,quantitative,objective,performance,randomsample" title="Quantitative Objective Performance: Text Input Speed in Words per Minute (WPM)">
			The average typing speed of a normal person is between 38 and 40 words per minute (this means around 190-200 characters per minute). Nevertheless, professional typists, or professionals who spend a lot of their time writing texts on desktop devices, have a typing speed of 65 to 75 words per minute.
		</method>

		<method id="fittslaw" category="empirical,quantitative,objective,performance,randomsample" title="Quantitative Objective Performance: Fitts' Law (Throughput)">
			Fitts' law states that the amount of time required for a person to move a pointer (e.g., mouse cursor) to a target area is a function of the distance to the target divided by the size of the target. Thus, the longer the distance and the smaller the target's size, the longer it takes. According to Fitts' law, human movement can be modeled by analogy to the transmission of information denoted as <strong>throughput</strong> in bits/s. Fitts' popular model has been widely adopted in numerous research areas, including kinematics, human factors, and human-computer interaction (HCI).
			<p><strong>1D Fitts' law task</strong>: MacKenzie, I. S. (1992). <a href='https://doi.org/10.1207/s15327051hci0701_3' target='_blank'>Fitts' law as a research and design tool in human-computer interaction.</a> Human-computer interaction, 7(1), 91-139.</p>
			<p><strong>2D Fitts' law task</strong>: MacKenzie, I. S., and Buxton, W. (1992, June). <a href='https://dl.acm.org/doi/abs/10.1145/142750.142794' target='_blank'>Extending Fitts' law to two-dimensional tasks.</a> In Proceedings of the SIGCHI conference on Human factors in computing systems (pp. 219-226).</p>
			<p><strong>3D Fitts' law task</strong>: Cha, Y., and Myung, R. (2013).  <a href='https://www.sciencedirect.com/science/article/abs/pii/S0169814113000723' target='_blank'>Extended Fitts' law for 3D pointing tasks using 3D target arrangements.</a> International Journal of Industrial Ergonomics, 43(4), 350-355.</p>
		</method>
		<method id="taskcompletiontime" category="summative,empirical,quantitative,objective,performance" title="Quantitative Objective Performance: Task Completion Time (TCT)">
			A measure of the time it takes a user to perform a task (from start to finish). This is a typical metric in usability evaluation. Please note that time-related measures are typically log-transformed for statistic evaluation.
			<a href='https://www.sciencedirect.com/topics/computer-science/task-completion-time' target='_blank'>Task Completion Time</a>
		</method>

		<method id="criticalreview" category="literature,narrative" title="A Critical Literature Review">
			Aims to demonstrate writer has extensively researched literature and critically evaluated its quality. Goes beyond mere description to include degree of analysis and conceptual innovation. Typically results in hypothesis or model. Seeks to identify significant items in the field.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://instr.iastate.libguides.com/c.php?g=885773' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>Up to author</li><li><strong>Review Resources: </strong>Up to author</li>
				<li><strong>Team: </strong>One author</li><li><strong>Searching strategy: </strong>Any or none</li>
				<li><strong>Appraisal: </strong>No formal quality assessment. Attempts to evaluate according to contribution.</li><li><strong>Results synthesis: </strong>Typically narrative, perhaps conceptual or chronological.</li>
				<li><strong>Analysis: </strong>Significant component: seeks to identify conceptual contribution to embody existing or derive new theory.</li>
			</ul>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="rapidreview" category="literature,narrative" title="A Rapid Literature Review">
			Assessment of what is already known about a policy or practice issue, by using systematic review methods to search and critically appraise existing research.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://instr.iastate.libguides.com/c.php?g=885773' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>2 - 6 months</li><li><strong>Review Resources: </strong>Up to author</li>
				<li><strong>Team: </strong>One author</li><li><strong>Searching strategy: </strong>Any or none</li>
				<li><strong>Appraisal: </strong>Time-limited. Attempts to evaluate according to contribution.</li><li><strong>Results synthesis: </strong>Typically narrative and tabular.</li>
				<li><strong>Analysis: </strong>Quantities of literature and overall quality/direction of effect of literature.</li>
			</ul>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="stateoftheartreview" category="literature,narrative" title="A State-of-the-Art Literature Review">
			Tend to address more current matters in contrast to other combined retrospective and current approaches. May offer new perspectives on issue or point out area for further research. Aims for comprehensive searching of current literature.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://instr.iastate.libguides.com/c.php?g=885773' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>Up to author</li><li><strong>Review Resources: </strong>Up to author</li>
				<li><strong>Team: </strong>One author</li><li><strong>Searching strategy: </strong>Any or none</li>
				<li><strong>Appraisal: </strong>No formal quality assessment.</li><li><strong>Results synthesis: </strong>Typically narrative, may have tabular accompaniment.</li>
				<li><strong>Analysis: </strong>Current state of knowledge and priorities for future investigation and research.</li>
			</ul>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="narrativereview" category="literature,narrative" title="A Narrative Literature Review">
			Traditional (narrative) literature reviews provide a broad overview of a research topic with <strong>no clear methodological approach</strong>. Information is collected and interpreted unsystematically with subjective summaries of findings. Authors aim to describe and discuss the literature from a contextual or theoretical point of view. Although the reviews may be conducted by topic experts, due to preconceived ideas or conclusions, they could be subject to bias. This sort of literature review can be appropriate if you have a broad topic area, are working on your own, or have time constraints.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://journal.emwa.org/writing-for-lay-audiences/writing-narrative-style-literature-reviews/' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>1 - 4 weeks</li><li><strong>Review Resources: </strong>Up to author</li>
				<li><strong>Team: </strong>One or more authors</li><li><strong>Searching strategy: </strong>Any or none</li>
				<li><strong>Appraisal: </strong>Any or none</li><li><strong>Results synthesis: </strong>Narrative commentary</li>
				<li><strong>Analysis: </strong>Summary of selected articles</li>
			</ul>
			Ferrari R. <a href='https://journal.emwa.org/writing-for-lay-audiences/writing-narrative-style-literature-reviews/' target='_blank'>Writing narrative style literature reviews</a>. Medical Writing. 2015 Dec 1;24(4):230-5.

			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="systematicreviewPRISMA" category="literature,systematic" title="A Systematic Literature Review: PRISMA Method">
			PRISMA is an evidence-based minimum set of items for reporting in systematic reviews and meta-analyses. PRISMA primarily focuses on the reporting of reviews evaluating the effects of interventions, but can also be used as a basis for reporting systematic reviews with objectives other than evaluating interventions.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://prisma-statement.org/Default.aspx' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>4-6 weeks or more depending on: number of databases, complexity, number of papers</li>
				<li><strong>Review Resources: </strong>Appropriate research databases for the research question</li>
				<li><strong>Team: </strong>Two or more people required for screening</li>
				<li><strong>Searching strategy: </strong>Exhaustive - aiming for all data published</li>
				<li><strong>Appraisal: </strong>Quality assessment involved</li>
				<li><strong>Results synthesis: </strong>Typically narrative or tabular</li>
				<li><strong>Analysis: </strong>what is known; recommendations for practice; what remains unknown; uncertainty around findings; recommendations for future research; taxonomies; classification schemes; methodological overview; ...</li>
			</ul>
			<a href='https://prisma-statement.org/PRISMAStatement/CitingAndUsingPRISMA' target='_blank'>PRISMA Statement and Citing PRISMA</a>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="systematicreviewKitchenham" category="literature,systematic" title="A Systematic Literature Review: Kitchenham Procedure">
			Appropriate for software engineering researchers and a mean of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Aim is to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guideline presented was derived from three existing guidelines used by medical researchers. The guideline has been adapted to reflect the specific problems of software engineering research. The guideline covers three phases of a systematic review: (1) planning the review, (2) conducting the review and (3) reporting the review. It does not consider the impact of question type on the review procedures, nor does it specify in detail mechanisms needed to undertake meta-analysis.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://www.inf.ufsc.br/~aldo.vw/kitchenham.pdf' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>4-6 weeks or more depending on: number of databases, complexity, number of papers</li>
				<li><strong>Review Resources: </strong>Appropriate research databases for the research question</li>
				<li><strong>Team: </strong>Two or more people required for screening</li>
				<li><strong>Searching strategy: </strong>Exhaustive - aiming for all data published</li>
				<li><strong>Appraisal: </strong>Quality assessment involved</li>
				<li><strong>Results synthesis: </strong>Typically narrative or tabular</li>
				<li><strong>Analysis: </strong>what is known; recommendations for practice; what remains unknown; uncertainty around findings; recommendations for future research; taxonomies; classification schemes; methodological overview; ...</li>
			</ul>
			Kitchenham, B. (2004). <a href="https://www.researchgate.net/profile/Barbara-Kitchenham/publication/228756057_Procedures_for_Performing_Systematic_Reviews/links/618cfae961f09877207f8471/Procedures-for-Performing-Systematic-Reviews.pdf">Procedures for performing systematic reviews</a>. Keele, UK, Keele University, 33(2004), 1-26.
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="systematicreviewAPISSER" category="literature,systematic" title="A Systematic Literature Review: The APISSER Methodology">
			Based on the well-known and proven PRISMA methodology from medical sciences, APISSER adapts and enhances the method to follow a task-oriented engineering flow and to be supported by customized tools. APISSER takes a step forward towards standardizing the methodology for executing systematic literature reviews in engineering by proposing a tool-supported and task-oriented engineering flow methodology to execute systematic literature reviews in engineering.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://ieeexplore.ieee.org/ielx7/6287639/9668973/09698182.pdf' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>4-6 weeks or more depending on: number of databases, complexity, number of papers</li>
				<li><strong>Review Resources: </strong>Appropriate research databases for the research question</li>
				<li><strong>Team: </strong>One or (better) two people required for screening</li>
				<li><strong>Searching strategy: </strong>Completeness of searching determined by scope constraints, often broad, often iterative</li>
				<li><strong>Appraisal: </strong>No formal quality assessment</li>
				<li><strong>Results synthesis: </strong>Typically tabular with some narrative commentary</li>
				<li><strong>Analysis: </strong>Characterizes quantity and breadth of literature. Attempts to specify the viability of more focused reviews</li>
			</ul>
			Castillo, S., and Grbovic, P. (2022).  IEEE Access, 10, 23700-23707. <a href='https://ieeexplore.ieee.org/ielx7/6287639/9668973/09698182.pdf' target='_blank'>The APISSER Methodology for Systematic Literature Reviews in Engineering</a>, 13(3), 141-146.
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="scopingreviewJBI" category="literature,scoping" title="A Scoping Review: The JBI Method">
			The JBI guidance for scoping reviews includes additional guidance on several methodological issues, such as when a scoping review is (or is not) appropriate, and how to extract, analyze, and present results, and provides clarification for implications for practice and research. Furthermore, it is aligned with the PRISMA-ScR to ensure consistent reporting.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://rgu-repository.worktribe.com/preview/998095/PETERS%202020%20Updated%20methodological.pdf' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>4-6 weeks or more depending on: number of databases, complexity, number of papers</li>
				<li><strong>Review Resources: </strong>Appropriate research databases for the research question</li>
				<li><strong>Team: </strong>One or (better) two people required for screening</li>
				<li><strong>Searching strategy: </strong>Completeness of searching determined by scope constraints, often broad, often iterative</li>
				<li><strong>Appraisal: </strong>No formal quality assessment</li>
				<li><strong>Results synthesis: </strong>Typically tabular with some narrative commentary</li>
				<li><strong>Analysis: </strong>Characterizes quantity and breadth of literature. Attempts to specify the viability of more focused reviews</li>
			</ul>
			<a href='https://rgu-repository.worktribe.com/preview/998095/PETERS%202020%20Updated%20methodological.pdf' target='_blank'>JBI Review Guide</a>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="scopingreviewPRISMASc" category="literature,scoping" title="A Scoping Review: The PRISMA-Sc Method">
			PRISMA-SC is a PRISMA extension for scoping reviews. The checklist contains 20 essential reporting items and 2 optional items to include when completing a scoping review. Scoping reviews serve to synthesize evidence and assess the scope of literature on a topic. Among other objectives, scoping reviews help determine whether a systematic review of the literature is warranted.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://journals.lww.com/ijebh/fulltext/2015/09000/guidance_for_conducting_systematic_scoping_reviews.5.aspx' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>4-6 weeks or more depending on: number of databases, complexity, number of papers</li>
				<li><strong>Review Resources: </strong>Appropriate research databases for the research question</li>
				<li><strong>Team: </strong>One or (better) two people required for screening</li>
				<li><strong>Searching strategy: </strong>Completeness of searching determined by scope constraints, often broad, often iterative</li>
				<li><strong>Appraisal: </strong>No formal quality assessment</li>
				<li><strong>Results synthesis: </strong>Typically tabular with some narrative commentary</li>
				<li><strong>Analysis: </strong>Characterizes quantity and breadth of literature. Attempts to specify the viability of more focused reviews</li>
			</ul>
			<a href='https://prisma-statement.org/PRISMAStatement/CitingAndUsingPRISMA' target='_blank'>PRISMA Statement and Citing PRISMA</a>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>

		<method id="metareviewPRISMASc" category="literature,metaanalysis" title= "A Meta-Analysis: The PRISMA Method">
			PRISMA is an evidence-based minimum set of items for reporting in systematic reviews and meta-analyses. PRISMA primarily focuses on the reporting of reviews evaluating the effects of interventions, but can also be used as a basis for reporting systematic reviews with objectives other than evaluating interventions.
			<ul>
				<li><strong>Review Guidelines: </strong><a href='https://prisma-statement.org/Default.aspx' target='_blank'>Click here</a></li>
				<li><strong>Review Time: </strong>4-6 weeks or more depending on: number of databases, complexity, number of papers</li>
				<li><strong>Review Resources: </strong>Appropriate research databases for the research question</li>
				<li><strong>Team: </strong>Two or more people required for screening</li>
				<li><strong>Searching strategy: </strong>Exhaustive - aiming for all data published</li>
				<li><strong>Appraisal: </strong>Quality assessment involved</li>
				<li><strong>Results synthesis: </strong>Typically narrative or tabular</li>
				<li><strong>Analysis: </strong>what is known; recommendations for practice; what remains unknown; uncertainty around findings; recommendations for future research; taxonomies; classification schemes; methodological overview; ...</li>
			</ul>
			<a href='https://prisma-statement.org/PRISMAStatement/CitingAndUsingPRISMA' target='_blank'>PRISMA Statement and Citing PRISMA</a>
			Grant, M.J. and Booth, A. (2009).  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1471-1842.2009.00848.x" target='_blank'>A typology of reviews: An analysis of 14 review types and associated methodologies.</a> Health Information and Libraries Journal, 26(2), 91-108.
		</method>
	</measures>
</research>
